# PanGu Drug Model - Optimized Configuration for High Performance
# This configuration is optimized for maximum processing speed and throughput

# Data Configuration - Optimized for Performance
data:
  dataset_path: "data/standard/train"  # Training dataset
  train_dataset_path: "data/standard/train"
  val_dataset_path: "data/standard/val"
  test_dataset_path: "data/standard/test"
  max_length: 128
  batch_size: 64  # Increased for better GPU utilization
  use_streaming: true  # Essential for large datasets
  cache_in_memory: false  # Disabled for memory efficiency
  prefetch_factor: 4  # Improved data loading pipeline
  num_workers: 8  # Parallel data loading

# Data Processing Configuration - for basic processing (process_data.py)
processing:
  raw_data_path: "data/raw"
  processed_data_path: "data/processed"
  chunk_size: 50000  # Optimized chunk size for memory/speed balance
  memory_limit_gb: 15.0
  
  # High-Performance Parallel Processing
  num_workers: 8  # Utilize all available CPU cores
  use_multiprocessing: true
  
  # Performance Optimizations
  use_memory_mapping: true  # Enable mmap for large files
  use_compression: true  # Enable LZ4 compression for storage
  progress_update_interval: 1000  # Reduce progress bar overhead
  expected_molecules: 10000000  # For Bloom filter optimization

# Data Standardization Configuration - High Performance Settings for molecular standardization (data_standardize.py)
standardize:
  standard_data_path: "data/standard"  # Final standardized data for training
  min_molecular_weight: 100.0  # Standard drug-like range
  max_molecular_weight: 600.0
  allowed_atoms: ["C", "N", "O", "S", "P", "F", "Cl", "Br", "I", "H"]
  
  # High-Performance Parallel Processing
  num_workers: 8  # Utilize all available CPU cores
  use_multiprocessing: true
  
  # Split ratios
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  
  # Standardization options
  remove_salts: true
  neutralize_charges: true
  canonicalize: true
  chunk_size: 50000
  progress_update_interval: 1000

# Model Architecture - Optimized for Performance
model:
  # Encoder parameters - Balanced for speed and quality
  num_node_features: 6
  hidden_dim: 256
  num_encoder_layers: 8  # Reduced from 10 for faster training
  num_encoder_heads: 8
  num_selected_layers: 8  # All layers for maximum information
  
  # Decoder parameters - Optimized
  num_decoder_layers: 6
  num_decoder_heads: 8
  latent_dim: 128
  
  # Training parameters
  output_dim: null  # Will be set based on vocabulary size
  dropout: 0.1  # Regularization

# Training Configuration - High Performance
training:
  learning_rate: 1e-4
  num_epochs: 100  # Increased for better convergence
  beta: 0.001  # KL divergence weight
  gradient_clip: 1.0
  gradient_accumulation_steps: 2  # Reduced for faster updates
  
  # Advanced optimization
  warmup_steps: 2000  # Increased warmup for stability
  scheduler: "cosine"  # Cosine annealing for better convergence
  weight_decay: 1e-4
  early_stopping_patience: 15  # Increased patience
  min_epochs: 20  # Minimum training epochs
  
  # Performance optimizations
  mixed_precision: true  # Enable AMP for speed
  compile_model: true  # PyTorch 2.0 compilation
  gradient_checkpointing: false  # Disabled for speed (enable if memory limited)

# Paths
paths:
  log_dir: "runs/pangu_optimized"
  checkpoint_path: "checkpoints/pangu_optimized.pt"
  best_model_path: "checkpoints/pangu_best.pt"
  
# Evaluation Configuration
evaluation:
  num_samples: 1000  # Increased for better evaluation
  device: "cuda"
  batch_size: 128  # Larger batch for evaluation
  
# System Configuration - Performance Focused
system:
  device: "cuda"
  mixed_precision: true  # Enable automatic mixed precision
  num_workers: 8  # Parallel processing
  pin_memory: true  # Pin memory for faster GPU transfer
  persistent_workers: true  # Keep workers alive between epochs
  
  # Memory optimizations
  max_memory_gb: 16.0  # Maximum memory usage
  memory_cleanup_interval: 100  # Cleanup every N batches
  
  # Performance monitoring
  profile_memory: false  # Disable for production
  profile_compute: false  # Disable for production
  log_gpu_memory: true  # Monitor GPU memory usage

# Advanced Performance Settings
performance:
  # Data loading optimizations
  dataloader_pin_memory: true
  dataloader_persistent_workers: true
  dataloader_prefetch_factor: 4
  
  # Model optimizations
  use_fused_adam: true  # Use fused Adam optimizer
  use_scaled_dot_product_attention: true  # PyTorch 2.0 optimized attention
  
  # Memory optimizations
  empty_cache_interval: 50  # Clear GPU cache every N steps
  max_split_size_mb: 512  # Control memory fragmentation
  
  # Compilation settings (PyTorch 2.0)
  torch_compile: true
  compile_mode: "default"  # Options: default, reduce-overhead, max-autotune
  
# Logging Configuration
logging:
  level: "INFO"
  log_interval: 100  # Log every N steps
  save_interval: 1000  # Save checkpoint every N steps
  eval_interval: 5000  # Evaluate every N steps
  
  # Metrics to track
  track_gpu_memory: true
  track_throughput: true
  track_loss_components: true
  
# Hyperparameter Search Configuration
hyperparameter_search:
  enabled: false
  method: "optuna"  # Options: optuna, random, grid
  n_trials: 50
  
  # Search space
  learning_rate_range: [1e-5, 1e-3]
  batch_size_options: [32, 64, 128]
  hidden_dim_options: [128, 256, 512]
  num_layers_range: [4, 10]

# Experimental Features
experimental:
  # Advanced optimizations (use with caution)
  use_torch_dynamo: false  # Experimental compilation
  use_memory_efficient_attention: true  # Memory-efficient attention
  use_gradient_compression: false  # Gradient compression for distributed training
  
  # Data processing experiments
  use_async_data_loading: false  # Experimental async loading
  use_smart_batching: true  # Dynamic batch sizing
  
# Environment Variables (for reference)
# CUDA_VISIBLE_DEVICES=0  # Use specific GPU
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  # Memory management
# OMP_NUM_THREADS=8  # OpenMP threads