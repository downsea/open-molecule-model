# PanGu Drug Model Configuration
# Including data processing parameters

# Data Configuration
data:
  dataset_path: "data/processed"
  max_length: 128
  batch_size: 32  # Standard batch size
  use_streaming: true  # Use streaming dataset to avoid loading all data
  cache_in_memory: false  # Disable memory caching for large datasets

# Data Processing Configuration
processing:
  raw_data_path: "data/raw"
  processed_data_path: "data/processed"
  chunk_size: 100000  # Molecules per processing chunk
  memory_limit_gb: 15.0
  min_molecular_weight: 10.0
  max_molecular_weight: 1000.0
  allowed_atoms: ["C", "N", "O", "S", "P", "F", "Cl", "Br", "I", "H"]
  
  # Parallel processing
  num_workers: 8  # Number of parallel processing workers
  use_multiprocessing: true
  
  # Split ratios
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  
  # Processing options
  remove_salts: true
  neutralize_charges: true
  canonicalize: true
  deduplicate: true

# Model Architecture
model:
  # Encoder parameters
  num_node_features: 6
  hidden_dim: 256
  num_encoder_layers: 10
  num_encoder_heads: 8
  num_selected_layers: 8  # Number of encoder layers to concatenate for latent
  
  # Decoder parameters
  num_decoder_layers: 6
  num_decoder_heads: 8
  latent_dim: 128
  
  # Training parameters
  output_dim: null  # Will be set based on vocabulary size

# Training Configuration
training:
  learning_rate: 1e-4
  num_epochs: 10
  beta: 0.001  # KL divergence weight
  gradient_clip: 1.0
  gradient_accumulation_steps: 4  # Accumulate gradients for larger effective batch size
  
# Paths
paths:
  log_dir: "runs/pangu_drug_model"
  checkpoint_path: "checkpoints/pangu_drug_model.pt"
  
# Evaluation Configuration
evaluation:
  num_samples: 100
  device: "cuda"
  
# Optimization
optimization:
  warmup_steps: 1000
  scheduler: "cosine"  # cosine, step, plateau
  weight_decay: 1e-4
  
# System
system:
  device: "cuda"
  mixed_precision: true  # Enable automatic mixed precision
  num_workers: 4
  pin_memory: true  # Pin memory for faster GPU transfer
  persistent_workers: true  # Keep workers alive between epochs